# ğŸ¥¦ Vegetable Supply Chain Data Pipeline

An **end-to-end production-grade Data Engineering project** that simulates a real-world vegetable supply chain.  
Built entirely on **Dockerized microservices** with **CI/CD, orchestration, monitoring, and business insights**.  

This project proves readiness for **real data engineering roles** by combining ingestion, processing, ML, dashboards, monitoring, and DevOps best practices.

---

## ğŸš€ Project Architecture

### 1. Data Ingestion
- **Source**: CSV files with vegetable prices & wastage data
- **Technology**: Kafka Producers & Consumers (Python)
- **Bronze Layer**: Raw immutable data in PostgreSQL warehouse

### 2. Data Processing â€“ Medallion Architecture
- **Bronze â†’ Silver**: Cleaning, validation & enrichment  
- **Silver â†’ Gold**: Business metrics + ML model for wastage prediction  
- **Database**: PostgreSQL (acts as warehouse)

### 3. Orchestration & Automation
- **Apache Airflow DAGs** schedule & manage ETL jobs:  
  - `bronze_to_silver_transformation`  
  - `silver_to_gold_transformation`  
  - `vegetable_pipeline_master`

### 4. Business Intelligence
- **Streamlit** â†’ interactive dashboards for daily analysis  
- **Grafana + Prometheus** â†’ executive dashboards with real-time KPIs  

### 5. DevOps & CI/CD
- **Docker** â†’ every service containerized (Kafka, Airflow, Postgres, Streamlit, Grafana, Prometheus)  
- **Single Docker Network** â†’ all services communicate seamlessly  
- **GitHub Actions CI/CD** â†’ automated testing & deployment for business metrics  

---

## ğŸ“Š Tech Stack

- **Languages**: Python, SQL, PySpark  
- **Streaming**: Apache Kafka  
- **Warehouse**: PostgreSQL  
- **Orchestration**: Apache Airflow  
- **Visualization**: Streamlit, Grafana  
- **Monitoring**: Prometheus  
- **DevOps**: Docker, Docker Compose, GitHub Actions  

---

## ğŸ³ Dockerized Architecture

- Each component runs in its own container:  
  - `jupyter` â€“ experimentation & CSV preprocessing  
  - `kafka` & `zookeeper` â€“ streaming backbone  
  - `postgres` â€“ warehouse  
  - `airflow-webserver`, `airflow-scheduler` â€“ orchestration  
  - `streamlit-app` â€“ business dashboard  
  - `grafana`, `prometheus` â€“ monitoring & alerting  

- **Infrastructure-as-Code**:  
  - `docker-compose.core.yml` â†’ core stack  
  - `docker-compose.airflow.yml` â†’ orchestration  
  - `docker-compose.streamlit.yml` â†’ BI  
  - `docker-compose.monitoring.yml` â†’ monitoring  
  - `prometheus.yml` â†’ Prometheus config  

---
## ğŸ“Œ Skills Demonstrated

- **Data Engineering**: Kafka (streaming), PostgreSQL (warehouse), Medallion Architecture (Bronze â†’ Silver â†’ Gold)
- **Orchestration**: Apache Airflow DAGs for ETL automation
- **Business Intelligence**: Streamlit dashboards for operational insights
- **Monitoring**: Grafana + Prometheus for system + business metrics
- **DevOps**: Docker, Docker Compose, GitHub Actions CI/CD
- **Programming**: Python, SQL, PySpark
- **Machine Learning**: Basic ML model integrated into Gold layer
- 
## ğŸ“¸ Screenshots

### Airflow â€“ Orchestration
![Airflow DAG](screenshots/airflow.png)

### Streamlit â€“ Business Dashboards
![Streamlit Dashboard 1](screenshots/streamlit(1).png)  
![Streamlit Dashboard 2](screenshots/streamlit(2).png)  
![Streamlit Dashboard 3](screenshots/streamlit(3).png)

### Monitoring â€“ Grafana & Prometheus
![Monitoring 1](screenshots/monitoring(1).png)  
![Monitoring 2](screenshots/monitoring(2).png)  
![Monitoring 3](screenshots/monitoring(3).png)

### GitHub CI/CD
![CI/CD](screenshots/github.png)

---

## âœ… Key Features

- **Real-time ingestion** â†’ Kafka â†’ Bronze  
- **Data cleaning & enrichment** â†’ Silver  
- **Business metrics + ML** â†’ Gold  
- **End-to-end orchestration** â†’ Airflow DAGs  
- **Executive insights** â†’ Streamlit + Grafana  
- **Monitoring & Alerts** â†’ Prometheus  
- **CI/CD** â†’ automated validation & deployment with GitHub Actions  
- **100% Dockerized** â†’ consistent, production-like environment  

---

## ğŸš€ Business Metrics Monitoring CI/CD

Automated **CI/CD pipeline for business metrics** in the vegetable supply chain.

### ğŸ“Š Metrics Monitored
- Bronze, Silver, Gold layer row counts  
- Average wastage percentage  
- Total avoidable loss cost (INR)  
- Top vegetables by wastage  

### ğŸš€ CI/CD Pipeline
**Continuous Integration (CI)**  
- Automatic testing on every push  
- Business logic validation  
- Database connectivity tests  

**Continuous Deployment (CD)**  
- Automatic deployment to production  
- Zero-downtime updates  
- Health verification  

### ğŸ› ï¸ Setup
```bash
# Install dependencies
pip install -r requirements.txt

# Run tests
pytest tests/ -v

# Start core services (Kafka, Postgres, Jupyter)
docker-compose -f docker-compose.core.yml up -d

# Start Airflow
docker-compose -f docker-compose.airflow.yml up -d

# Start Streamlit Dashboard
docker-compose -f docker-compose.streamlit.yml up -d

# Start Monitoring Stack
docker-compose -f docker-compose.monitoring.yml up -d



